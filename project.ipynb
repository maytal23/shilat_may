{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from datetime import date, timedelta\n",
    "import time\n",
    "\n",
    "# This function generates a list of dates between two dates.\n",
    "def daterange(start_date, end_date):\n",
    "    for n in range(int((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)\n",
    "\n",
    "# This line sets the path to the ChromeDriver executable and the url for the website we want to scrape.\n",
    "url = 'https://midrug.safenet.co.il/app/'\n",
    "\n",
    "# This line creates a new instance of the ChromeDriver object.\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# This line opens the Midrug website.\n",
    "driver.get(url)\n",
    "\n",
    "# This line defines the start and end dates for the data scraping.\n",
    "start_date = date(2020, 1, 1)\n",
    "end_date = date(2023, 5, 22)\n",
    "\n",
    "# This line creates an empty list to store the data.\n",
    "lst = []\n",
    "\n",
    "# This loop iterates over the dates between the start and end dates.\n",
    "for single_date in daterange(start_date, end_date):\n",
    "\n",
    "    # This line selects the \"1\" crowd from the dropdown menu.\n",
    "    crowd = Select(driver.find_element(By.ID, 'Crowd'))\n",
    "    crowd.select_by_value('1')\n",
    "\n",
    "    # This line clears the text input field for the date.\n",
    "    date = driver.find_element(By.ID, 'TheDate')\n",
    "    date.clear()\n",
    "\n",
    "    # This line enters the date into the text input field.\n",
    "    date.send_keys(single_date.strftime(r\"%d%m%Y\"))\n",
    "\n",
    "    # This line clicks the search button.\n",
    "    search = driver.find_element(By.XPATH, '//*[@id=\"DataPlus\"]/table/tbody/tr[6]/td[2]/input')\n",
    "    search.click()\n",
    "\n",
    "    # This line tries to find the table with the ratings data.\n",
    "    try:\n",
    "        time.sleep(0.5)\n",
    "        table = driver.find_element(By.ID, \"Rep2\")\n",
    "\n",
    "    # This line catches the `NoSuchElementException` exception and sets the table to `None`.\n",
    "    except NoSuchElementException:\n",
    "        table = None\n",
    "        pass\n",
    "\n",
    "    # This line only adds the data to the list if the table exists.\n",
    "    if table is not None:\n",
    "        data = pd.read_html(table.get_attribute(\"outerHTML\"), encoding='windows-1255')[0]\n",
    "        data_lst = data.values.tolist()\n",
    "        for row in data_lst:\n",
    "            lst.append(row)\n",
    "\n",
    "# This line creates a Pandas DataFrame from the list of data.\n",
    "df = pd.DataFrame(lst, columns=data.columns)\n",
    "\n",
    "# This line writes the DataFrame to a CSV file.\n",
    "df.to_csv('text.csv', encoding='windows-1255')\n",
    "\n",
    "# This line prints the first five rows of the DataFrame.\n",
    "print(df.head())\n",
    "\n",
    "# This line quits the ChromeDriver object.\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "# Read the csv file.\n",
    "df = pd.read_csv('text.csv', encoding='windows-1255')\n",
    "df.head()\n",
    "\n",
    "# Create a Datetime column from Date and Time.\n",
    "df['Datetime'] = df['תאריך'] + ' ' + df['שעת השידור המקורית']\n",
    "df.head()\n",
    "\n",
    "df['Datetime'] = pd.to_datetime(df['Datetime'], format='%d/%m/%Y %H:%M')\n",
    "df = df.drop(['תאריך', 'שעת השידור המקורית'], axis=1)\n",
    "df.head()\n",
    "\n",
    "# Renaming the columns.\n",
    "df = df.rename(columns={'Unnamed: 0': 'ID',\n",
    "                 'דירוג': 'Ranking',\n",
    "                 'שם התוכנית': 'TV_Show',\n",
    "                 'ערוץ/משדר': 'Channel',\n",
    "                 'משך בדקות': 'Duration',\n",
    "                 'שיעורי צפייה במשקי בית (%)': 'Ranking_Percent',\n",
    "                 'אלפי משקי בית': 'Households'})\n",
    "df.head()\n",
    "\n",
    "# Perform feature extraction from the Datetime column we made earlier.\n",
    "df['Month'] = df['Datetime'].dt.month\n",
    "df['Week'] = df['Datetime'].dt.week\n",
    "df['Day'] = df['Datetime'].dt.day\n",
    "df['Hour'] = df['Datetime'].dt.hour\n",
    "df['Minute'] = df['Datetime'].dt.minute\n",
    "df['Day_of_week'] = df['Datetime'].dt.day_of_week\n",
    "df.head()\n",
    "\n",
    "# Scaling Households.\n",
    "df['Households'] = df['Households'].map(lambda x : x * 1000)\n",
    "df.head()\n",
    "\n",
    "df.describe()\n",
    "\n",
    "# There are no nulls in the dataset.\n",
    "df.info()\n",
    "\n",
    "# Number of TV shows in the dataset.\n",
    "len(df['TV_Show'].unique())\n",
    "\n",
    "# Number of channels.\n",
    "len(df['Channel'].unique())\n",
    "\n",
    "\n",
    "# What channels do we have in the dataset?\n",
    "print(df['Channel'].unique())\n",
    "\n",
    "# How many shows from each channel?\n",
    "counts = df['Channel'].value_counts()\n",
    "\n",
    "# We can see that by far, the most shows are in Keshet 12 and Reshet 13, so much that the other channels are insignificant in this dataset.\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(counts.index, counts.values)\n",
    "ax.set_xlabel('Channel')\n",
    "ax.set_ylabel('count')\n",
    "plt.show()\n",
    "\n",
    "# We drop the rows that don't belong to Keshet 12 or Reshet 13 based on the visualization above.\n",
    "df = df.drop(df[(df['Channel'] != 'קשת 12') & (df['Channel'] != 'רשת 13')].index)\n",
    "\n",
    "# We dropped 1,180 rows.\n",
    "df.info()\n",
    "\n",
    "# We are indeed left with only Keshet 12 and Reshet 13 in the dataset.\n",
    "df['Channel'].unique()\n",
    "\n",
    "# Dropping TV shows that have less than 5 episodes, because they can skew the results.\n",
    "value_counts = df['TV_Show'].value_counts()\n",
    "to_drop = value_counts[value_counts < 5].index\n",
    "df = df[~df['TV_Show'].isin(to_drop)]\n",
    "\n",
    "# We dropped 1,878 rows.\n",
    "df.info()\n",
    "\n",
    "# Indeed we have shows that are 5 episodes and up.\n",
    "df['TV_Show'].value_counts()\n",
    "\n",
    "# Creating a correlation matrix and plotting it. we can see households and ranking percent are basically the same thing.\n",
    "corr = df.corr()\n",
    "\n",
    "# Plot the correlation matrix\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "plt.show()\n",
    "\n",
    "# Dropping Households because we want to predict Ranking_Percent\n",
    "df = df.drop(['ID', 'Households'], axis=1)\n",
    "df.head()\n",
    "\n",
    "# Plotting Ranking percent by hours in the day. we can see that the highest rating is at 21:00 O'clock.\n",
    "group_hr = df.groupby('Hour')['Ranking_Percent'].mean()\n",
    "\n",
    "plt.xticks(range(24))\n",
    "plt.xlim(0, 24)\n",
    "group_hr.plot(kind='line', figsize=(10, 10))\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Percentage')\n",
    "plt.title('Percentage variable over hours')\n",
    "plt.show()\n",
    "\n",
    "# Plotting Ranking percent by days of the month. we can see that the lowest rating is in the 10th of every month, Maybe salaries is the reason?.\n",
    "group_day = df.groupby('Day')['Ranking_Percent'].mean()\n",
    "\n",
    "group_day.plot(kind='line', figsize=(10, 10))\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Percentage')\n",
    "plt.title('Percentage variable over days')\n",
    "plt.show()\n",
    "\n",
    "# Plotting Ranking percent by months of the year. we can see that the lowest rating is in September, and the highest rating is in March.\n",
    "# Winter and summer maybe?\n",
    "group_month = df.groupby('Month')['Ranking_Percent'].mean()\n",
    "\n",
    "group_month.plot(kind='line', figsize=(10, 10))\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Percentage')\n",
    "plt.title('Percentage variable over months')\n",
    "plt.show()\n",
    "\n",
    "# Plotting Ranking percent by days of the week. we can see that the lowest rating is in Wednesday and the highest in Monday.\n",
    "group_dow = df.groupby('Day_of_week')['Ranking_Percent'].mean()\n",
    "\n",
    "group_dow.plot(kind='line', figsize=(10, 10))\n",
    "plt.xlabel('Day of week')\n",
    "plt.ylabel('Percentage')\n",
    "plt.title('Percentage variable over the days of the week')\n",
    "plt.show()\n",
    "\n",
    "# Plotting Ranking percent by Duration of a show. we can see spikes around 80 minutes, 100 minutes and 170-180 minutes.\n",
    "grouped_df = df.groupby('Duration')\n",
    "\n",
    "# Calculate the mean of the weight column for each group\n",
    "mean_rank_df = grouped_df['Ranking_Percent'].mean()\n",
    "\n",
    "# Plot the mean of the weight column for each group\n",
    "mean_rank_df.plot(kind='line', figsize=(10, 5))\n",
    "\n",
    "# Add labels to the axes\n",
    "plt.xlabel('TV_Show')\n",
    "plt.ylabel('Ranking_Percent')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Showing the 20 shows with the highest rating in the last 3 years. first is Hazamar Bamasechah, Second is Married at first sight etc..\n",
    "# Mostly reality shows.\n",
    "grouped_df = df.groupby('TV_Show')\n",
    "\n",
    "# Calculate the mean of the weight column for each group\n",
    "mean_rank_df = grouped_df['Ranking_Percent'].mean().nlargest(20)\n",
    "\n",
    "# Plot the mean of the weight column for each group\n",
    "mean_rank_df.plot(kind='bar', figsize=(10, 5))\n",
    "\n",
    "# Add labels to the axes\n",
    "plt.xlabel('TV_Show')\n",
    "plt.ylabel('Ranking_Percent')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Keshet has more rating than Reshet.\n",
    "grouped_df = df.groupby('Channel')\n",
    "\n",
    "# Calculate the mean of the weight column for each group\n",
    "mean_rank_df = grouped_df['Ranking_Percent'].mean()\n",
    "\n",
    "# Plot the mean of the weight column for each group\n",
    "mean_rank_df.plot(kind='bar', figsize=(10, 5))\n",
    "\n",
    "# Add labels to the axes\n",
    "plt.xlabel('Channel')\n",
    "plt.ylabel('Ranking_Percent')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Saving the processed dataset\n",
    "df.to_csv('processed.csv', encoding='windows-1255')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "import shap\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Read the csv file.\n",
    "df = pd.read_csv('processed.csv', encoding='windows-1255')\n",
    "df.head()\n",
    "\n",
    "# Drop unwanted columns for training.\n",
    "df = df.drop(['Unnamed: 0', 'Datetime'], axis=1)\n",
    "df.head()\n",
    "\n",
    "# Perform label encoding on the categorical variables, the model we chose can handle them automatically but we need it for the SHAP algorithm.\n",
    "encoders = {}\n",
    "\n",
    "for column in df.select_dtypes(include=[object]):\n",
    "    le = LabelEncoder()\n",
    "    df[column] = le.fit_transform(df[column])\n",
    "    encoders[column] = le\n",
    "\n",
    "    encoders['TV_Show'].classes_\n",
    "\n",
    "    # Seperate the dependent variable from the independent variables.\n",
    "y = df['Ranking_Percent']\n",
    "X = df.drop(['Ranking_Percent'], axis=1)\n",
    "\n",
    "# Split the data to train and test sets and making sure to have an equal representation of all the tv shows in the datasets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, stratify=X['TV_Show'], random_state=42)\n",
    "\n",
    "# Train a baseline model.\n",
    "model = lgb.LGBMRegressor()\n",
    "model.fit(X_train, y_train, categorical_feature='auto')\n",
    "\n",
    "# Get the predictions.\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluation metrics.\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False) # Root mean squared error.\n",
    "mae = mean_absolute_error(y_test, y_pred) # Mean Absolute Error\n",
    "r2 = r2_score(y_test, y_pred) # R2 Score\n",
    "\n",
    "rmse\n",
    "mae\n",
    "r2\n",
    "\n",
    "# Search spaces for Bayesian Search.\n",
    "search_spaces = {\n",
    "    'boosting_type': Categorical(['gbdt', 'dart']),\n",
    "    'num_leaves': Integer(20, 200),\n",
    "    'learning_rate': Real(0.01, 0.2, 'log-uniform'),\n",
    "    'n_estimators': Integer(50, 1000),\n",
    "    'max_depth': Integer(5, 50),\n",
    "    'min_data_in_leaf': Integer(10, 300),\n",
    "    'max_bin': Integer(100, 400),\n",
    "    'feature_fraction': Real(0.6, 1.0, 'uniform'),\n",
    "    'bagging_fraction': Real(0.6, 1.0, 'uniform'),\n",
    "    'bagging_freq': Integer(0, 20),\n",
    "    'min_sum_hessian_in_leaf': Real(0, 10),\n",
    "    'lambda_l1': Real(1e-10, 100, 'log-uniform'),\n",
    "    'lambda_l2': Real(1e-10, 100, 'log-uniform'),\n",
    "    'min_gain_to_split': Real(0, 0.7),\n",
    "}\n",
    "\n",
    "# Optimization.\n",
    "lgbm = lgb.LGBMRegressor(objective='regression', metric='mae')\n",
    "\n",
    "opt = BayesSearchCV(\n",
    "    lgbm,\n",
    "    search_spaces,\n",
    "    n_iter=50,\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "opt.fit(X_train, y_train, categorical_feature='auto')\n",
    "\n",
    "print(\"Best parameters found: \", opt.best_params_)\n",
    "\n",
    "\n",
    "# Training a regressor with the best hyperparameters.\n",
    "best = lgb.LGBMRegressor(**opt.best_params_)\n",
    "best.fit(X_train, y_train)\n",
    "\n",
    "# Function to print the evaluation metrics.\n",
    "def get_scores(y_true, y_pred):\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f'RMSE: {rmse}\\n MAE: {mae}\\n R2: {r2}')\n",
    "\n",
    "    # Training set evaluation.\n",
    "y_pred_train = best.predict(X_train)\n",
    "get_scores(y_train, y_pred_train)\n",
    "\n",
    "# Test set evaluation.\n",
    "y_pred_test = best.predict(X_test)\n",
    "get_scores(y_test, y_pred_test)\n",
    "\n",
    "# Create a SHAP  tree explainer instance, because LGBM is a tree based algorithm.\n",
    "explainer = shap.TreeExplainer(best)\n",
    "\n",
    "# Calculate SHAP values.\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Plot the SHAP values for the first instance\n",
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value, shap_values[0], X_test.iloc[0])\n",
    "\n",
    "# Get the TV show label.\n",
    "X_test['TV_Show'].iloc[0]\n",
    "\n",
    "# Get the TV show name.\n",
    "encoders['TV_Show'].classes_[144]\n",
    "\n",
    "# A SHAP summary plot showing the impact of every feature on the outcome.\n",
    "shap.summary_plot(shap_values, X.columns)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
